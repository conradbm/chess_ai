{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chess AI\n",
    "<hr>\n",
    "### Reinforcement Learning\n",
    "<hr>\n",
    "### Monte Carlo Method | Bellman Equation | Optimal Policy Iteration\n",
    "<hr>\n",
    "\n",
    "The following is a simulation that terminates after games.\n",
    "The objective is to train a black chess agent how to play chess purely based off the `Monte Carlo`, `Bellman's Equation`, and `Optimal Policy Iteration`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "<hr>\n",
    "## Simulation Setup\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "os.chdir(\"/Users/laurensuarez/Documents/blake/chess/data\")\n",
    "def random_action(theBoard, thePolicy, eps=0.1):\n",
    "    \n",
    "    # Current state\n",
    "    s=str(theBoard)\n",
    "    \n",
    "    #   ///////////////////\n",
    "    #  // If Unexplored //\n",
    "    # ///////////////////\n",
    "    if s not in list(thePolicy.keys()): \n",
    "        return np.random.choice(list(theBoard.legal_moves))\n",
    "    \n",
    "    #   ////////////////////\n",
    "    #  // Epsilon-greedy //\n",
    "    # ////////////////////\n",
    "    p=np.random.random() # [0,1]    \n",
    "    \n",
    "    #   /////////////\n",
    "    #  // Exploit //\n",
    "    # /////////////\n",
    "    if p < (1-eps):\n",
    "        selected=None\n",
    "        a=thePolicy[s]\n",
    "        for act in list(theBoard.legal_moves):\n",
    "            #print(str(act))\n",
    "            if str(act) == a:\n",
    "                selected=act\n",
    "                \n",
    "        #   /////////////\n",
    "        #  // Explore //\n",
    "        # //////////////\n",
    "        if selected is None:\n",
    "            # The possibility of being in a state that had an option\n",
    "            # but is now currently preoccupied by a piece, or check conditions, ect..\n",
    "            # hence, just move randomly, treat it as if we do not have an option.\n",
    "            return np.random.choice(list(theBoard.legal_moves))\n",
    "        else:\n",
    "            return selected\n",
    "        \n",
    "    #   /////////////\n",
    "    #  // Explore //\n",
    "    # //////////////\n",
    "    else:\n",
    "        return np.random.choice(list(theBoard.legal_moves))\n",
    "\n",
    "# Play game as black\n",
    "def play_game(theBoard, thePolicy, gamma=0.9):\n",
    "        \n",
    "    #   //////////////////////\n",
    "    #  // White Move First //\n",
    "    # //////////////////////\n",
    "    move=np.random.choice(list(theBoard.legal_moves))\n",
    "    theBoard.push(move)\n",
    "    \n",
    "    #   //////////////\n",
    "    #  // Game on! //\n",
    "    # //////////////\n",
    "    state_action_rewards=[]\n",
    "    while theBoard.is_game_over()==False:\n",
    "        \n",
    "        # Make intelligent move from policy (iteratively updated)\n",
    "        state=str(theBoard)\n",
    "        move=random_action(theBoard, thePolicy)\n",
    "        action=str(move)\n",
    "        state_action_rewards.append([state,action,0])\n",
    "        theBoard.push(move)\n",
    "    \n",
    "    # Allocate winner points\n",
    "    winner=newBoard.result()\n",
    "    if winner == '1-0':\n",
    "        # We lost\n",
    "        state_action_rewards[-1][2]=-1\n",
    "    elif winner == '0-1':\n",
    "        # We won!\n",
    "        state_action_rewards[-1][2]=1\n",
    "    else:\n",
    "        # Tie...\n",
    "        state_action_rewards[-1][2]=0.001\n",
    "        pass\n",
    "    \n",
    "    #   /////////////////////////////\n",
    "    #  // Bellman Update Equation //\n",
    "    # /////////////////////////////\n",
    "    G = 0\n",
    "    states_actions_returns = []\n",
    "\n",
    "    for s,a,r in reversed(state_action_rewards):\n",
    "\n",
    "        states_actions_returns.append((s, a, G))\n",
    "        G = r + gamma*G\n",
    "    states_actions_returns.reverse()\n",
    "    \n",
    "    return states_actions_returns\n",
    "    \n",
    "policy={}\n",
    "Q=defaultdict(dict)\n",
    "returns=defaultdict(list)\n",
    "def run_simulation(num_games=5000, write_file_mod=200):\n",
    "\n",
    "    #   //////////////////////\n",
    "    #  // Agent Containers //\n",
    "    # //////////////////////\n",
    "    policy={}\n",
    "    Q=defaultdict(dict)\n",
    "    returns=defaultdict(list)\n",
    "\n",
    "    #   //////////////////////\n",
    "    #  // Simulation Begin //\n",
    "    # //////////////////////\n",
    "    deltas=[]\n",
    "    seen_state_action_pairs=set()\n",
    "    for i in range(num_games):\n",
    "        \n",
    "        if i % write_file_mod == 0 and i != 0:\n",
    "            print(i)\n",
    "            custom_time=str(datetime.datetime.now()).replace(\".\",\"_\").replace(\":\",\"_\").replace(\" \", \"\")\n",
    "            if not os.path.exists(os.path.join(os.getcwd(),custom_time)):\n",
    "                os.makedirs(os.path.join(os.getcwd(),custom_time))\n",
    "            pickle_dump(Q, custom_time+\"/Q_\"+custom_time+\".pkl\")\n",
    "            pickle_dump(Q, custom_time+\"/policy_\"+custom_time+\".pkl\")\n",
    "            pickle_dump(Q, custom_time+\"/V_\"+custom_time+\".pkl\")\n",
    "        #Fresh board\n",
    "        newBoard=chess.Board()\n",
    "        \n",
    "        #For visualization\n",
    "        biggest_change=0\n",
    "        \n",
    "        #Collect 1 game sequence \n",
    "        state_action_returns_list=play_game(newBoard, policy)\n",
    "        \n",
    "        #   ///////////////////////////\n",
    "        #  // Value function update //\n",
    "        # ///////////////////////////\n",
    "        for s, a, G in state_action_returns_list:\n",
    "            sa = (s, a)\n",
    "            if sa in seen_state_action_pairs:\n",
    "                old_q=Q[s][a]\n",
    "            elif sa not in seen_state_action_pairs:\n",
    "                old_q=0\n",
    "                seen_state_action_pairs.add(sa)\n",
    "            else:\n",
    "                print(\"Error: (s,a) is niether been seen in the past nor has it not been seen. Huge problem.\")\n",
    "            returns[sa].append(G)\n",
    "            Q[s][a]=np.mean(returns[sa]) #rolling mean, Q now `learns`\n",
    "            biggest_change=max([biggest_change, np.abs(old_q - Q[s][a])])\n",
    "            deltas.append(biggest_change)\n",
    "            \n",
    "            \n",
    "        #   ///////////////////\n",
    "        #  // Policy Update //\n",
    "        # ///////////////////\n",
    "        for s in Q.keys():\n",
    "            keys=list(Q[s].keys())\n",
    "            vals=list(Q[s].values())\n",
    "            a = keys[np.argmax(vals)]\n",
    "            policy[s] = a\n",
    "    V = {}\n",
    "    for s in policy.keys():\n",
    "        V[s] = Q[s][1]\n",
    "    return Q, policy, V, deltas\n",
    "                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the sim\n",
    "### Plot for convergence visualization\n",
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "Q,policy,V,deltas=run_simulation(num_games=5000, write_file_mod=200)\n",
    "plt.plot(deltas)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "Confirm process working correctly.\n",
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "os.chdir(\"/Users/laurensuarez/Documents/blake/chess/data\")\n",
    "class MacOSFile(object):\n",
    "\n",
    "    def __init__(self, f):\n",
    "        self.f = f\n",
    "\n",
    "    def __getattr__(self, item):\n",
    "        return getattr(self.f, item)\n",
    "\n",
    "    def read(self, n):\n",
    "        # print(\"reading total_bytes=%s\" % n, flush=True)\n",
    "        if n >= (1 << 31):\n",
    "            buffer = bytearray(n)\n",
    "            idx = 0\n",
    "            while idx < n:\n",
    "                batch_size = min(n - idx, 1 << 31 - 1)\n",
    "                # print(\"reading bytes [%s,%s)...\" % (idx, idx + batch_size), end=\"\", flush=True)\n",
    "                buffer[idx:idx + batch_size] = self.f.read(batch_size)\n",
    "                # print(\"done.\", flush=True)\n",
    "                idx += batch_size\n",
    "            return buffer\n",
    "        return self.f.read(n)\n",
    "\n",
    "    def write(self, buffer):\n",
    "        n = len(buffer)\n",
    "        print(\"writing total_bytes=%s...\" % n, flush=True)\n",
    "        idx = 0\n",
    "        while idx < n:\n",
    "            batch_size = min(n - idx, 1 << 31 - 1)\n",
    "            print(\"writing bytes [%s, %s)... \" % (idx, idx + batch_size), end=\"\", flush=True)\n",
    "            self.f.write(buffer[idx:idx + batch_size])\n",
    "            print(\"done.\", flush=True)\n",
    "            idx += batch_size\n",
    "\n",
    "def pickle_dump(obj, file_path):\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        return pickle.dump(obj, MacOSFile(f), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def pickle_load(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        return pickle.load(MacOSFile(f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q=pickle_load(\"Q_2018-11-2421_43_58_744997.pkl\")\n",
    "policy=pickle_load(\"policy_2018-11-2421_43_58_744997.pkl\")\n",
    "V=pickle_load(\"V_2018-11-2421_43_58_744997.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h7h5': 1.9622177898881668e-14,\n",
       " 'f5f4': 2.776664336428095e-08,\n",
       " 'a6a5': 7.943796589169338e-19}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q[list(Q.keys())[8]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximation Functions\n",
    "\n",
    "Since we know the size of a dictionary in python is limited. The better strategy is to learn the value function using deep learning techniques, so we can obtain high quality predictions `f(state, params)=value`.\n",
    "\n",
    "We now seek to build a model that can predict the value function, rather than just storing them in a value dictionary with a policy.\n",
    "\n",
    "This integrates supervised learning into reinforcement learning.\n",
    "\n",
    "$$ error = (value_{true}-value_{pred})^2$$\n",
    "$$ error = (E[G(t) | S_t = s]-value_{pred})^2$$\n",
    "$$ error = (\\dfrac{1}{N}\\sum\\limits_{k=1}^n G_{i,s} - value_{pred})^2$$\n",
    "\n",
    "So each run in our simulation becomes a training point:\n",
    "\n",
    "$$ error = \\sum\\limits_{k=1}^n (y_i -\\hat{y_i})^2$$\n",
    "\n",
    "When using linear methods to find the transformation of a state, you must manually do feature engineering. However, when using non-linear methods such as deep neural networks (which are also differentiable), we can allow them to construct the features that are most important. So we continue with the simple linear case and construct features for our board states.\n",
    "\n",
    "One way to do this is to `one_hot_encode` by each of our states. A lot like time series data as a categorical variable. This blows up in dimensionality, so this really isn't any improvement. The positive to this is that you can clearly explain what the problem is, your model or your features. \n",
    "\n",
    "Another approach is to model your board as the raw data itself; a tensor. Construct a tensor transformation and make the data (8,8,14), representing each piece of the board and which piece currently occupies it (or if none do). This is also a challenge. \n",
    "\n",
    "Yet another approach is to apply `Taylor Series Expansion`, which contains the capability to approximate any real valued function using sum of polynomials. This could be done using `w=f(x,y,z)=x**2+y**2+z**2`. The possibilities are endless here if you are using a linear model.\n",
    "\n",
    "We can manually apply gradient descent to optimize our linear model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "os.chdir(\"/Users/laurensuarez/Documents/blake/chess/data\")\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, alpha=0.001):\n",
    "        self.theta=np.random.randn(4)/2\n",
    "        self.alpha=alpha\n",
    "    \n",
    "    def s2x(self, s):\n",
    "        piece_ratio=np.divide(np.sum([i.islower() for i in L]),np.sum(i.isupper() for i in L))\n",
    "        black_count=np.sum([i.islower() for i in L])\n",
    "        white_count=np.sum([i.isupper() for i in L])\n",
    "        bias=1.0\n",
    "        return np.array([piece_ratio,\n",
    "                         black_count,\n",
    "                         white_count,\n",
    "                         bias])\n",
    "    def predict(self, s):\n",
    "        x=self.s2x(s)\n",
    "        return self.theta.dot(x)\n",
    "    \n",
    "    def fit(self, V_hat, G):\n",
    "        self.theta+=self.alpha*(G - V_hat)*x\n",
    "\n",
    "    def set_alpha(self, newalpha):\n",
    "        self.alpha=newalpha\n",
    "        \n",
    "def random_action(theBoard, thePolicy, eps=0.1):\n",
    "    \n",
    "    # Current state\n",
    "    s=str(theBoard)\n",
    "    \n",
    "    #   ///////////////////\n",
    "    #  // If Unexplored //\n",
    "    # ///////////////////\n",
    "    if s not in list(thePolicy.keys()): \n",
    "        return np.random.choice(list(theBoard.legal_moves))\n",
    "    \n",
    "    #   ////////////////////\n",
    "    #  // Epsilon-greedy //\n",
    "    # ////////////////////\n",
    "    p=np.random.random() # [0,1]    \n",
    "    \n",
    "    #   /////////////\n",
    "    #  // Exploit //\n",
    "    # /////////////\n",
    "    if p < (1-eps):\n",
    "        selected=None\n",
    "        a=thePolicy[s]\n",
    "        for act in list(theBoard.legal_moves):\n",
    "            #print(str(act))\n",
    "            if str(act) == a:\n",
    "                selected=act\n",
    "                \n",
    "        #   /////////////\n",
    "        #  // Explore //\n",
    "        # //////////////\n",
    "        if selected is None:\n",
    "            # The possibility of being in a state that had an option\n",
    "            # but is now currently preoccupied by a piece, or check conditions, ect..\n",
    "            # hence, just move randomly, treat it as if we do not have an option.\n",
    "            return np.random.choice(list(theBoard.legal_moves))\n",
    "        else:\n",
    "            return selected\n",
    "        \n",
    "    #   /////////////\n",
    "    #  // Explore //\n",
    "    # //////////////\n",
    "    else:\n",
    "        return np.random.choice(list(theBoard.legal_moves))\n",
    "\n",
    "# Play game as black\n",
    "def play_game(theBoard, thePolicy, gamma=0.9):\n",
    "        \n",
    "    #   //////////////////////\n",
    "    #  // White Move First //\n",
    "    # //////////////////////\n",
    "    move=np.random.choice(list(theBoard.legal_moves))\n",
    "    theBoard.push(move)\n",
    "    \n",
    "    #   //////////////\n",
    "    #  // Game on! //\n",
    "    # //////////////\n",
    "    state_action_rewards=[]\n",
    "    while theBoard.is_game_over()==False:\n",
    "        \n",
    "        # Make intelligent move from policy (iteratively updated)\n",
    "        state=str(theBoard)\n",
    "        move=random_action(theBoard, thePolicy)\n",
    "        action=str(move)\n",
    "        state_action_rewards.append([state,action,0])\n",
    "        theBoard.push(move)\n",
    "    \n",
    "    # Allocate winner points\n",
    "    winner=newBoard.result()\n",
    "    if winner == '1-0':\n",
    "        # We lost\n",
    "        state_action_rewards[-1][2]=-1\n",
    "    elif winner == '0-1':\n",
    "        # We won!\n",
    "        state_action_rewards[-1][2]=1\n",
    "    else:\n",
    "        # Tie...\n",
    "        state_action_rewards[-1][2]=0.001\n",
    "        pass\n",
    "    \n",
    "    #   /////////////////////////////\n",
    "    #  // Bellman Update Equation //\n",
    "    # /////////////////////////////\n",
    "    G = 0\n",
    "    states_actions_returns = []\n",
    "\n",
    "    for s,a,r in reversed(state_action_rewards):\n",
    "\n",
    "        states_actions_returns.append((s, a, G))\n",
    "        G = r + gamma*G\n",
    "    states_actions_returns.reverse()\n",
    "    \n",
    "    return states_actions_returns\n",
    "    \n",
    "policy={}\n",
    "Q=defaultdict(dict)\n",
    "returns=defaultdict(list)\n",
    "def run_simulation_approx(num_games=5000, write_file_mod=200, alpha=0.001):\n",
    "\n",
    "    #   //////////////////////\n",
    "    #  // Agent Containers //\n",
    "    # //////////////////////\n",
    "    policy={}\n",
    "    Q=defaultdict(dict)\n",
    "    returns=defaultdict(list)\n",
    "\n",
    "    \n",
    "    #     ////////////////////////////////////////////\n",
    "    #    // Linear Value Function Approximation    //\n",
    "    #   // Kernel Function                        //\n",
    "    #  // Transform state (8,8,13) into vector   //\n",
    "    # ////////////////////////////////////////////\n",
    "    model=Model(alpha=alpha)\n",
    "    \n",
    "    #   //////////////////////\n",
    "    #  // Simulation Begin //\n",
    "    # //////////////////////\n",
    "    deltas=[]\n",
    "    seen_states=set()\n",
    "    t=1.0\n",
    "    for i in range(num_games):\n",
    "        \n",
    "        #Decaying learning rate\n",
    "        alpha/=t\n",
    "        model.set_alpha(alpha)\n",
    "        \n",
    "        if i % write_file_mod == 0 and i != 0:\n",
    "            print(i)\n",
    "            custom_time=str(datetime.datetime.now()).replace(\".\",\"_\").replace(\":\",\"_\").replace(\" \", \"\")\n",
    "            if not os.path.exists(os.path.join(os.getcwd(),custom_time)):\n",
    "                os.makedirs(os.path.join(os.getcwd(),custom_time))\n",
    "            pickle_dump(Q, custom_time+\"/Q_\"+custom_time+\".pkl\")\n",
    "            pickle_dump(Q, custom_time+\"/policy_\"+custom_time+\".pkl\")\n",
    "            pickle_dump(Q, custom_time+\"/V_\"+custom_time+\".pkl\")\n",
    "            \n",
    "            #Update learning decay rate\n",
    "            t+=0.01\n",
    "        \n",
    "        #Fresh board\n",
    "        newBoard=chess.Board()\n",
    "        \n",
    "        #For visualization\n",
    "        biggest_change=0\n",
    "        \n",
    "        #Collect 1 game sequence \n",
    "        state_action_returns_list=play_game(newBoard, policy)\n",
    "        \n",
    "        #   ///////////////////////////\n",
    "        #  // Value function update //\n",
    "        # ///////////////////////////\n",
    "        for s, a, G in state_action_returns_list:\n",
    "            \n",
    "            # For every state, regardless of whether we have seen\n",
    "            # it or not, we should be able to just use previous\n",
    "            # knowledge, or simply update from the past if we\n",
    "            # see the same state in context, so this is where\n",
    "            # I deviate a bit from the traditional algorithm.\n",
    "            old_theta=model.theta.copy()\n",
    "            x=model.s2x(s)\n",
    "            V_hat=model.predict(s)\n",
    "            model.fit(V_hat, G) #grad(V_hat w.r.t theta=x)\n",
    "            \n",
    "            biggest_change=max([biggest_change, np.abs(old_theta - theta).sum()])\n",
    "            seen_states.add(s)\n",
    "            deltas.append(biggest_change)\n",
    "            \n",
    "        \n",
    "    #   //////////////////\n",
    "    #  // Value Update //\n",
    "    # //////////////////\n",
    "    V = {}\n",
    "    for s in seen_states:\n",
    "        V[s] = model.theta.dot(s2x(s))\n",
    "    return V, theta, deltas\n",
    "                                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
