{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chess AI\n",
    "<hr>\n",
    "### Reinforcement Learning\n",
    "<hr>\n",
    "### Monte Carlo Method | Bellman Equation | Optimal Policy Iteration\n",
    "<hr>\n",
    "\n",
    "The following is a simulation that terminates after games.\n",
    "The objective is to train a black chess agent how to play chess purely based off the `Monte Carlo`, `Bellman's Equation`, and `Optimal Policy Iteration`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "<hr>\n",
    "## Simulation Setup\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "os.chdir(\"/Users/laurensuarez/Documents/blake/chess/data\")\n",
    "def random_action(theBoard, thePolicy, eps=0.1):\n",
    "    \n",
    "    # Current state\n",
    "    s=str(theBoard)\n",
    "    \n",
    "    #   ///////////////////\n",
    "    #  // If Unexplored //\n",
    "    # ///////////////////\n",
    "    if s not in list(thePolicy.keys()): \n",
    "        return np.random.choice(list(theBoard.legal_moves))\n",
    "    \n",
    "    #   ////////////////////\n",
    "    #  // Epsilon-greedy //\n",
    "    # ////////////////////\n",
    "    p=np.random.random() # [0,1]    \n",
    "    \n",
    "    #   /////////////\n",
    "    #  // Exploit //\n",
    "    # /////////////\n",
    "    if p < (1-eps):\n",
    "        selected=None\n",
    "        a=thePolicy[s]\n",
    "        for act in list(theBoard.legal_moves):\n",
    "            #print(str(act))\n",
    "            if str(act) == a:\n",
    "                selected=act\n",
    "                \n",
    "        #   /////////////\n",
    "        #  // Explore //\n",
    "        # //////////////\n",
    "        if selected is None:\n",
    "            # The possibility of being in a state that had an option\n",
    "            # but is now currently preoccupied by a piece, or check conditions, ect..\n",
    "            # hence, just move randomly, treat it as if we do not have an option.\n",
    "            return np.random.choice(list(theBoard.legal_moves))\n",
    "        else:\n",
    "            return selected\n",
    "        \n",
    "    #   /////////////\n",
    "    #  // Explore //\n",
    "    # //////////////\n",
    "    else:\n",
    "        return np.random.choice(list(theBoard.legal_moves))\n",
    "\n",
    "# Play game as black\n",
    "def play_game(theBoard, thePolicy, gamma=0.9):\n",
    "        \n",
    "    #   //////////////////////\n",
    "    #  // White Move First //\n",
    "    # //////////////////////\n",
    "    move=np.random.choice(list(theBoard.legal_moves))\n",
    "    theBoard.push(move)\n",
    "    \n",
    "    #   //////////////\n",
    "    #  // Game on! //\n",
    "    # //////////////\n",
    "    state_action_rewards=[]\n",
    "    while theBoard.is_game_over()==False:\n",
    "        \n",
    "        # Make intelligent move from policy (iteratively updated)\n",
    "        state=str(theBoard)\n",
    "        move=random_action(theBoard, thePolicy)\n",
    "        action=str(move)\n",
    "        state_action_rewards.append([state,action,0])\n",
    "        theBoard.push(move)\n",
    "    \n",
    "    # Allocate winner points\n",
    "    winner=newBoard.result()\n",
    "    if winner == '1-0':\n",
    "        # We lost\n",
    "        state_action_rewards[-1][2]=-1\n",
    "    elif winner == '0-1':\n",
    "        # We won!\n",
    "        state_action_rewards[-1][2]=1\n",
    "    else:\n",
    "        # Tie...\n",
    "        state_action_rewards[-1][2]=0.001\n",
    "        pass\n",
    "    \n",
    "    #   /////////////////////////////\n",
    "    #  // Bellman Update Equation //\n",
    "    # /////////////////////////////\n",
    "    G = 0\n",
    "    states_actions_returns = []\n",
    "\n",
    "    for s,a,r in reversed(state_action_rewards):\n",
    "\n",
    "        states_actions_returns.append((s, a, G))\n",
    "        G = r + gamma*G\n",
    "    states_actions_returns.reverse()\n",
    "    \n",
    "    return states_actions_returns\n",
    "    \n",
    "policy={}\n",
    "Q=defaultdict(dict)\n",
    "returns=defaultdict(list)\n",
    "def run_simulation(num_games=5000, write_file_mod=200):\n",
    "\n",
    "    #   //////////////////////\n",
    "    #  // Agent Containers //\n",
    "    # //////////////////////\n",
    "    policy={}\n",
    "    Q=defaultdict(dict)\n",
    "    returns=defaultdict(list)\n",
    "\n",
    "    #   //////////////////////\n",
    "    #  // Simulation Begin //\n",
    "    # //////////////////////\n",
    "    deltas=[]\n",
    "    seen_state_action_pairs=set()\n",
    "    for i in range(num_games):\n",
    "        \n",
    "        if i % write_file_mod == 0 and i != 0:\n",
    "            print(i)\n",
    "            custom_time=str(datetime.datetime.now()).replace(\".\",\"_\").replace(\":\",\"_\").replace(\" \", \"\")\n",
    "            if not os.path.exists(os.path.join(os.getcwd(),custom_time)):\n",
    "                os.makedirs(os.path.join(os.getcwd(),custom_time))\n",
    "            pickle_dump(Q, custom_time+\"/Q_\"+custom_time+\".pkl\")\n",
    "            pickle_dump(Q, custom_time+\"/policy_\"+custom_time+\".pkl\")\n",
    "            pickle_dump(Q, custom_time+\"/V_\"+custom_time+\".pkl\")\n",
    "        #Fresh board\n",
    "        newBoard=chess.Board()\n",
    "        \n",
    "        #For visualization\n",
    "        biggest_change=0\n",
    "        \n",
    "        #Collect 1 game sequence \n",
    "        state_action_returns_list=play_game(newBoard, policy)\n",
    "        \n",
    "        #   ///////////////////////////\n",
    "        #  // Value function update //\n",
    "        # ///////////////////////////\n",
    "        for s, a, G in state_action_returns_list:\n",
    "            sa = (s, a)\n",
    "            if sa in seen_state_action_pairs:\n",
    "                old_q=Q[s][a]\n",
    "            elif sa not in seen_state_action_pairs:\n",
    "                old_q=0\n",
    "                seen_state_action_pairs.add(sa)\n",
    "            else:\n",
    "                print(\"Error: (s,a) is niether been seen in the past nor has it not been seen. Huge problem.\")\n",
    "            returns[sa].append(G)\n",
    "            Q[s][a]=np.mean(returns[sa]) #rolling mean, Q now `learns`\n",
    "            biggest_change=max([biggest_change, np.abs(old_q - Q[s][a])])\n",
    "            deltas.append(biggest_change)\n",
    "            \n",
    "            \n",
    "        #   ///////////////////\n",
    "        #  // Policy Update //\n",
    "        # ///////////////////\n",
    "        for s in Q.keys():\n",
    "            keys=list(Q[s].keys())\n",
    "            vals=list(Q[s].values())\n",
    "            a = keys[np.argmax(vals)]\n",
    "            policy[s] = a\n",
    "    V = {}\n",
    "    for s in policy.keys():\n",
    "        V[s] = Q[s][1]\n",
    "    return Q, policy, V, deltas\n",
    "                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the sim\n",
    "### Plot for convergence visualization\n",
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "Q,policy,V,deltas=run_simulation(num_games=5000, write_file_mod=200)\n",
    "plt.plot(deltas)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "Confirm process working correctly.\n",
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "os.chdir(\"/Users/laurensuarez/Documents/blake/chess/data\")\n",
    "class MacOSFile(object):\n",
    "\n",
    "    def __init__(self, f):\n",
    "        self.f = f\n",
    "\n",
    "    def __getattr__(self, item):\n",
    "        return getattr(self.f, item)\n",
    "\n",
    "    def read(self, n):\n",
    "        # print(\"reading total_bytes=%s\" % n, flush=True)\n",
    "        if n >= (1 << 31):\n",
    "            buffer = bytearray(n)\n",
    "            idx = 0\n",
    "            while idx < n:\n",
    "                batch_size = min(n - idx, 1 << 31 - 1)\n",
    "                # print(\"reading bytes [%s,%s)...\" % (idx, idx + batch_size), end=\"\", flush=True)\n",
    "                buffer[idx:idx + batch_size] = self.f.read(batch_size)\n",
    "                # print(\"done.\", flush=True)\n",
    "                idx += batch_size\n",
    "            return buffer\n",
    "        return self.f.read(n)\n",
    "\n",
    "    def write(self, buffer):\n",
    "        n = len(buffer)\n",
    "        print(\"writing total_bytes=%s...\" % n, flush=True)\n",
    "        idx = 0\n",
    "        while idx < n:\n",
    "            batch_size = min(n - idx, 1 << 31 - 1)\n",
    "            print(\"writing bytes [%s, %s)... \" % (idx, idx + batch_size), end=\"\", flush=True)\n",
    "            self.f.write(buffer[idx:idx + batch_size])\n",
    "            print(\"done.\", flush=True)\n",
    "            idx += batch_size\n",
    "\n",
    "def pickle_dump(obj, file_path):\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        return pickle.dump(obj, MacOSFile(f), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def pickle_load(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        return pickle.load(MacOSFile(f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q=pickle_load(\"Q_2018-11-2421_43_58_744997.pkl\")\n",
    "policy=pickle_load(\"policy_2018-11-2421_43_58_744997.pkl\")\n",
    "V=pickle_load(\"V_2018-11-2421_43_58_744997.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h7h5': 1.9622177898881668e-14,\n",
       " 'f5f4': 2.776664336428095e-08,\n",
       " 'a6a5': 7.943796589169338e-19}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q[list(Q.keys())[8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
